{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632bfe2d-f5cf-4b1e-8b77-3ec47c9e54db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from transformers import AutoImageProcessor\n",
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eceb1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"WANDB_PROJECT\"]=\"pc-rvlcdip-custom-training1\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\"\n",
    "model_checkpoint = \"microsoft/dit-base-finetuned-rvlcdip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5da5e7-8d9e-40dd-8623-91034d2fec8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"custom-rvlcdip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06afaa0-a8d4-4cc5-8b2b-f9bfbc9174dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639d5622-484c-478e-925a-6eff86fd1933",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982417a3-6b80-449b-afd7-f84bae0dc912",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[\"train\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc51d4e-ed20-4eab-baa9-4f061e6f6b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a980e279-665c-44d0-9f5b-af2ae52bc130",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"].features[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d30a5dc-74c4-4dde-bc95-5ee0e4ad3d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"letter\",\n",
    "    1: \"form\",\n",
    "    2: \"email\",\n",
    "    3: \"handwritten\",\n",
    "    4: \"advertisement\",\n",
    "    5: \"scientific report\",\n",
    "    6: \"scientific publication\",\n",
    "    7: \"specification\",\n",
    "    8: \"file folder\",\n",
    "    9: \"news article\",\n",
    "    10: \"budget\",\n",
    "    11: \"invoice\",\n",
    "    12: \"presentation\",\n",
    "    13: \"questionnaire\",\n",
    "    14: \"resume\",\n",
    "    15: \"memo\"\n",
    "}\n",
    "\n",
    "label2id = {\n",
    "    \"advertisement\": 4,\n",
    "    \"budget\": 10,\n",
    "    \"email\": 2,\n",
    "    \"file folder\": 8,\n",
    "    \"form\": 1,\n",
    "    \"handwritten\": 3,\n",
    "    \"invoice\": 11,\n",
    "    \"letter\": 0,\n",
    "    \"memo\": 15,\n",
    "    \"news article\": 9,\n",
    "    \"presentation\": 12,\n",
    "    \"questionnaire\": 13,\n",
    "    \"resume\": 14,\n",
    "    \"scientific publication\": 6,\n",
    "    \"scientific report\": 5,\n",
    "    \"specification\": 7\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8f9d94-273b-419e-abf9-abf46a990d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor  = AutoImageProcessor.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb1f6b0-a24f-403e-bd80-5d1b25a2fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "if \"height\" in image_processor.size:\n",
    "    size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "    crop_size = size\n",
    "    max_size = None\n",
    "elif \"shortest_edge\" in image_processor.size:\n",
    "    size = image_processor.size[\"shortest_edge\"]\n",
    "    crop_size = (size, size)\n",
    "    max_size = image_processor.size.get(\"longest_edge\")\n",
    "\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(crop_size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(crop_size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    example_batch[\"pixel_values\"] = [\n",
    "        train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b589a545-098f-43fd-b51a-1b993c04be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "train_ds = splits['train']\n",
    "val_ds = splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb0f26-fafa-4669-a663-c1443c4ade3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.set_transform(preprocess_train)\n",
    "val_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d27dd4-f07c-4178-9b4a-ad02104288c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec89a7b4-29db-4417-94da-3cd11281fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199398bc-774f-4d87-a82b-4a32b2b4f837",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "batch_size = 64\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-custom-rvlcdip\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=100,\n",
    "    warmup_ratio=0.1,\n",
    "    report_to=\"wandb\",\n",
    "    logging_steps=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de5b049-a2d7-4979-8a03-9d0ba02586dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638b0d7d-a823-4052-929f-89296ef382a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4d9de-0651-49a2-ab91-f393a728c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7884219",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
